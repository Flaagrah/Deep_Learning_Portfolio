Generative Adversarial Networks (GAN's) train a model that is able to generate data that
imitates a dataset. They do this by pitting a "Generative" model (model that
generates data) against a "Discriminative" model (model that attempts to distingush
between real data and data generated by the Generative model). The discriminator
is trained to distinguish between fake and real data by being given labeled samples
from both the generated and the real dataset. Simulataneously, the Generator is trained
to make the discriminator respond to its data as if it was real. This creates a perpetual
dual where the generator becomes better at mimicking actual data and the discriminator
becomes better at distinguishing the real data from the generated data. Thus, the generator
becomes effective in mimicking the real dataset. GAN's are based on the following paper
"Generative Adversarial Nets", https://papers.nips.cc/paper/5423-generative-adversarial-nets.pdf.



This Project contains two GAN's. One of them trains on MNIST data to produce digits
and the other trains on CIFAR-10 horse images to produce
images of horses. The MNIST GAN is from a tutorial of a Udemy course titled "https://www.udemy.com/complete-guide-to-tensorflow-for-deep-learning-with-python/"
The CIFAR-10 GAN uses some of the code from the tutorial and is derived from the model
of the following project (not associated with me):
"https://github.com/4thgen/DCGAN-CIFAR10"

The CIFAR-10 GAN trains a generator and a discriminator on horse images from the CIFAR-10
dataset. 

CIFAR-10 Generator:
The generator uses reverse convolutional layers to derive a 3072 dimensional output from
the given input. I used small kernel sizes small strides and a large number of filters (128)
to capture many 2x2 permutations of pixel values that might occur in the images. I used the
leaky_relu activation instead of the relu activation to avoid sparsity in the network which
would limit the flexibility of the generator.

CIFAR-10 Discriminator:
Batch normalization was used in the discriminator since it would experience large shifts in
data distribution. The following paper discusses batch normalization and how it reduces the
effect of large shifts in distribution:

"Batch Normalization: Accelerating Deep Network Training
by Reducing Internal Covariate Shift", "https://arxiv.org/pdf/1502.03167.pdf".

Large kernel sizes (5,5) captures features of horses which can be used to determine whether
there is a horse in the picture.

Sigmoid cross entropy is used as the loss for the discriminator as that is the loss that was
used in the original GAN research paper.

The images shown in the GAN directory or of images generated by the model at each epoch.